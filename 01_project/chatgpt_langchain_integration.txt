Time to start working on our first application. And this first app is not gonna be the PDF project we just spoke about. We're gonna do that application a little bit later. Right now, we're gonna keep it really simple and just go through a quick introduction to some basics around LangChain and making use of it. So this first program is gonna be executed from our command line. So we'll run something like python main.py. Now we're going to give it two different arguments. We're gonna give it a task and a language. The goal of this program is to use ChatGPT to generate a very simple code snippet that does whatever the task is in the provided language. So in this case, I would expect ChatGPT to generate a code snippet that prints out numbers from 1 to 10, and it should generate that program in the Python language. And that's it. So like I said, pretty simple and straightforward. To build this project, we're gonna need a couple of things. Super basic here. Of course, we're gonna need Python 3 installed. I'm going to assume you already have Python up and ready to go. If you do not, there's a ton of guides online on how to get Python installed. We're going to have to install a package called OpenAI. This is what is going to allow us to communicate programmatically with ChatGPT. We're also going to, of course, need to install LangChain. We're gonna install both these using pip, package manager, and I'll walk you through that install process in just a moment. We're going to need a OpenAI API key so we can access ChatGPT programmatically. To generate this key and make use of it, I'm gonna put some directions in a text lecture right after this one. The process of generating that key changes every now and then, and it's just a lot easier to keep it up to date if it's in text format. So that's why it's gonna be in a text lecture right after this. And then, of course, we need a code editor and a terminal. I'm using VSCode, personally. You can use anything you want. And of course, you're gonna need a terminal as well. Once again, you can use anything you want. Okay, let's get to it. To get started, I'm gonna open up my terminal. I'm inside of a workspace directory, so this can be anywhere on your computer. I'm going to create a new project folder, and I'm gonna call this, how about pycode, 'cause we're using Python to generate some code. I'll then change into that directory. And I'm gonna open up my code editor inside this folder. I've already opened my code editor, just to save a little bit of time. Here it is right here. So I'm inside of that pycode folder. Once I've opened up my code editor inside of here, I'm also gonna get a terminal window open inside of here as well. So you can use the terminal window we just had open, or you can open up a terminal inside of your code editor. So I'm gonna do the latter just so you can see my code and my terminal at the same time very easily. And then finally for this video, we'll create a new file called main.py. Inside there, I'm just gonna print out hi there, save it, and I'm just gonna make sure I can run that thing successfully without any errors or anything like that. So yep, definitely looks good. All right, so we've got the basics put together, time to start to learn more about LangChain and OpenAI in just a second.

In the next lecture right before this one, you should have seen directions on generating an API key with OpenAI. So here's my key right here. I'm going to copy it. Take it back over to my code editor. I'm going to delete the print statement. And I'll say API key is my key. And I'm going to paste it in as a string. Now just so you know, this API key is extremely sensitive and we never want to accidentally share it with anyone else. Pasting it directly inside of a source code file like this is definitely not good practice. I'm going to show you how we can better secure this key in just a moment. But for right now, I really just want to make use of link chain. So we're going to come back and secure the key. And I'm going to put a comment in here of secure this key. So I don't forget about that. Okay. So now let's talk about the plan. How are we going to write some code. How are we going to get access to OpenAI's API and make use of ChatGPT or something similar? Well, here's what we're going to do in this video. We're going to write out a little bit of code and make use of link chain to just generate some text in the simplest, most basic way possible. And the way I'm going to show you is not super often used on professional projects. The reason I'm showing you this simple, kind of simplistic way of generating some text with link chain is that I really want to hammer home the fact that one of the big goals of Link Chain is to give you these super interchangeable tools. So we're going to write out a little bit of code. See the simple way to generate some text. We're going to point out some downsides to it. And then we'll refactor into the much more commonly used method. So let's get to it. Okay, so in Main.py at the very top of the file we're going to import some stuff from Lang. So we'll say from lang dot import open AI. And notice we got a capital A and capital I on there underneath my API key I'll make a new variable called lm. I'll say open AI and I'll pass in a keyword argument of open AI underscore API underscore key. And I'll assign to that the API key variable. And that's pretty much it. We can now use this thing right here. This object that I'm calling LM to generate some text. So I might say something like LM write a very very short poem. I'll assign the result of that to a variable called result, and I'll print that up. Let's save the file. And then again at our terminal we can run this program with a Python main.py. I'm going to run that. And then after a brief pause I should see a very short poem up here. Okay, so we've made use of link chain. We generated some text and it's a really simple way of doing this. But like I said well there's some downsides to this approach. So the next video we're going to point out some issues with this super simplistic model we've put together. And we're going to start to do a refactor.

In this video, I wanna clarify how this program is working. We're then gonna take a look at a couple of different diagrams, introduce a new concept, and then use this new concept to improve our program. So let's get to it. So here's what's going on inside of our program right now. We've got main.py. Inside there we are importing a LangChain package. In turn, LangChain internally, is importing another package called openai. Whenever we feed some text into this llm object right here, it is sent off in an HTTP request over to OpenAI servers. That's where the language model is hosted. Some text is generated and then it's sent back in response to us. The only reason I show you this diagram is to just make sure it is clear that right now we are not running a language model locally on your computer. Instead it is hosted off on OpenAI servers. Okay, so that's just a quick idea of what's going on inside this file. So now a couple of diagrams. I want to introduce a big new topic to you. We're gonna use this new topic to really dramatically improve our program. To get started, I first want to remind you, something we discussed is a big goal inside of LangChain previously, and introduce another big goal of LangChain. So the first big goal, and again, we already spoke about this, LangChain wants to give us tools to automate all this text generation stuff. Another big goal is that LangChain wants to make it easy for us to connect all these different tools together, super easily. So lemme show you a couple diagrams just to remind you why step number one is so important, or big goal number one is so important. This is a diagram of what's gonna go on inside of our application. So we're gonna run our program from the terminal. We're gonna parse out some inputs. We're then going to take those inputs and join them together into a string of sorts. And that string might be something like, write a short, blank, program that will achieve some task. Once we've generated a prompt, we'll then feed it into our language model and then hopefully get out some output. Now, I want you to imagine that at some point in time in the future, maybe we are running this program and we're super happy with it, but eventually maybe OpenAI goes outta business, or maybe there's another alternative out there that just plain is better. And so we would want to make sure that we could very easily swap out this very critical part of our program. So I would want to be able to take out open AI super easily and put in say, a locally-hosted Llama instance, or maybe Claude, which is another language model provided by a company called Anthropic. So we've kind of gone over this. It's really important for us, in LangChain, to be able to swap out these super-critical parts. Now I'm gonna show you another diagram that's gonna expand on the second goal right here. It's really important for us to be able to connect these different tools together super easily. So let's imagine that at some point in time in the future, we decide our current program is working really well, but we want to expand it in some way. Maybe we want to add in some kind of error checking. So maybe after generating some output code over here, we then want to feed it into another prompt, something that says, maybe, make sure the snippet doesn't have any bugs in it, and then put the code right there. And once we generate the string, we would then feed it into another language model called, maybe Bard, which would tell us the code looks good, or maybe it looks bad, or whatever else. And we say, look at this flow. You're gonna start to notice that we kind of get this repeatable pattern in it. We've got some inputs that get fed into a prompt, and that prompt then gets fed into a language model that's gonna give us some output. And if we want to join this up with another processing pipeline, we wanna take that output right there, feed it into a prompt, feed that into a language model, and then get our next output. So we can start to realize that it looks like this entire text generation stuff really revolves around having some input, prompt, model, output, and then take that output, use it as input to a prompt, to a model, to output. Okay, so now we're going to connect these two ideas together. So it's very clear that we want to be able to swap models or maybe prompt templates or whatever else in and out of these pipelines really easily, and we want to be able to connect these pipelines together really easily as well. So the big tool that we get inside of LangChain to make this happen is something called a chain. A chain is the absolute foundation of just about everything inside of LangChain. And we're going to be building a ton of chains inside this course. So lemme give you some details around chains really quickly. A chain is a Python class provided by LangChain. So this is a class that's already been written for us. We're going to import it from LangChain. We're gonna create an instance of it, and we're gonna pass in some configuration options. We use these chains to make reusable text generation pipelines. A typical application might have multiple chains inside of it. We then can take these different chains and connect them together to make a really interesting application. The two critical parts of a chain is something called a prompt template, and an LLM or a language model. Once we have created the chain, we then feed in some input and we get back some output. Now lemme go into some more detail. I'm gonna tell you exactly what's going on with the prompt template language model and then clarify some things around the input and output. So first, the prompt template. The prompt template is kind of like just a fancy thing that's gonna build up a string. That's pretty much it. So it is a class, it is going to be given a string that's kind of like our template for the prompt we want to send off to the language model. Whenever we make a prompt template, we must declare the variables that our template needs ahead of time. So we have to say that in order to run this prompt template, we need a language variable and a task variable. So that's for our particular application, we need language and task, but you might have your own custom prompt templates you're gonna build in the future on your own, and those will probably have totally different named variables that are going to be used inside the template. The other thing we have to provide to a chain whenever we create it is a language model. So this is going to be, again, a class. It can be something that's gonna allow us to access ChatGPT or Bard or Claude or Llama, or pretty much any language model, or really anything that generates text, period. So those are the two things that are required to make a chain. We connect them together inside of a chain and then we're going to feed the chain some input, and we're going to get, in response, some output. So now I wanna clarify what these input and output things really are, because this is, believe it or not, as simple as input and output sounds, understanding this next part is probably the most critical part for understanding what a chain is. So it's gonna seem really simple, but it's really important to sit down and just acknowledge, oh yeah, this is something really important. So whenever we create a chain, if we want to run the chain, we're going to give it a set of inputs. And this is very often going to be a dictionary. It can also be a set of keyword arguments. This dictionary must contain all the values for each of the different variables that are required in the prompt template. So that's our prompt template right there. Our prompt template requires language and task. So in order to run our chain, we must provide a language and a task. Once the chain runs, it's gonna give us some outputs. And here's, again, a little bit of a confusing part. We're going to get back a dictionary that's gonna contain the inputs that were fed into the chain, and an additional key called text, and text is gonna be pretty much the generated text. It's gonna be whatever was made by the language model, this key of text inside the dictionary, it can be customized so it doesn't have to be text. We're gonna very often customize it. In this case, maybe it would make a lot more sense to call this thing code, because it's like the generated code, text is just the default, that's all. Now, once we've built a chain that can take in some inputs as a dictionary and then give us some outputs as dictionary, it starts to become really easy to connect these different chains together. So let's revisit this idea over here where we have our first little, actually, let's look at this one. We've got our first little code generation step and then some secondary code checking step. To build something like this, we would create two separate chains, maybe Chain A and Chain B. Chain A over here would have inputs of say language and task. They would go into this template, they would then be sent off to language model, and then we would get outputs of language, task, and then maybe instead of calling the default output, which would be text, maybe we customize it to be code instead. We've then got Chain B over here, which needs inputs of language and code. It would then take those and put them to this template that says, make sure the following, I dunno, maybe Python snippet works, and then a little bit of a Python snippet right there. Once that thing runs, we would then get outputs of language, code, and then whatever generated text, to connect these two chains together, you might notice that these outputs right here, the names of them kind of line up exactly with the inputs of Chain B. Task is not actually required, that's an extra one, but Chain B needs an input called language and code. So we can imagine that we take these outputs right here and we feed them in as inputs to Chain B. And that's quite simply how we connect these chains together, is through very consistent and clever naming of all these inputs and outputs from our chains. Whew, okay! So this is a lot to throw out you in one single video. And I apologize that this video is so long, but I'll be honest with you, I sincerely believe that I probably just saved you a lot of hours of wondering what's going on inside of LangChain 'cause this is really gonna set the foundation for a ton of stuff to follow. I do not expect you to memorize anything inside this video, this is really just a big taste of what's to come. So as you might guess, we're gonna pause right here. We're gonna go back to our project and we're going to refactor this thing to use a chain. And as we build this chain, we're gonna see this idea of prompt templates and input variables and output variables and all this stuff start to come up really quickly.

Let's refactor our code to make use of a chain as opposed to the very simple direct access to a model that we're currently doing. In total, we're going to need to make a chain and a prompt template. We've already created the language model. That's what we already added inside of the main.py file. So let's get to it. To get started at the very top, I'm gonna add in an import from langchain.prompts. I will import PromptTemplate, and from langchain.chains, I will import LLMChain. Next up, after we create our language model right here, I'm going to make my prompt template. I'm gonna call it code_prompt. This will be a prompt template, and we're gonna first put in a keyword argument here called template. And this is going to be the template for the prompt that we're going to eventually generate. So we will put in something like, write a very short and then language and curly braces function that will task in curly braces. So again, that's our template. When we eventually run our chain, as you probably imagine, we're gonna take language and task and replace 'em with whatever got passed into our program's command line arguments. Whenever we specify some variable interpolation inside the string, we must also declare these as input variables. So a second keyword argument here, input variables, it's going to be a list. And as strings, we're gonna list out all the different variables that are required to make this prompt template work correctly. And so we kinda discussed this a moment ago. Remember this is this part right here. Whenever we make our prompt template, we have to list out the variables that our prompt needs. So we're gonna put in here language and task. All right, next up we're going to use the language model that we put together and this prompt template, which we call the code prompt to make our chain. So I'm gonna put in a new variable of code chain that will be an LLMChain. We'll put in keyword arguments to specify the language model to use and the prompt to use. And then finally, I'm gonna replace or update what we have down here to run our chain. We'll say result is code_chain, we're gonna call that. And then we're going to put in a dictionary that contains all the input variables that are required to make our prompt template work. So for you and I, we've got a prompt template that requires a language and a task. That means that we must put in a dictionary here that has a key of language. And right now I'll hardcode this as Python and a key of task. And I'll hardcode this says, how about to return a list of numbers. So now if we print the result, the result is going to be, once again, we saw this in a diagram right here. Yeah, result is going to be a dictionary and it's gonna contain all of our inputs along with the output as well as a key marked text. And again, that key can be customized if we want to, but for right now, we'll leave it as the default, just so we understand that it is the default. So let's save this. I'm gonna open up my terminal and I'll run my program with a main.py. And there we go, so there is our output, and as you can see very plainly, yep, definitely a dictionary. It has our inputs of language and task, and then our output of text. And there's our generated code. Very often you're going to see in documentation if you care just about that generated text, because you know very often we already know what the language and the task is. So very often you'll see that people just make use of results at text like so. And if we make that change and save this and run it again, now our print statement is just going to be the generated code by itself. Very good.

Our program works, and I'm super happy about that, but remember, we did not really want to have these hard-coded values for language and task. We wanted to be able to provide them as command line arguments whenever we run our program. So something that looks like that. Adding in support for command line argument parsing is super easy in Python. Let me show you how it's done really quickly. At the very top of the file, I'm going to import argparse. I'm going to create a parser object with argparse.ArgumentParser. We're then going to tell this parser that we expect to receive two command line arguments. One will be called task, the other will be called language. So we'll say parser.add_argument "--task", and then I'm gonna provide a default value here. So if I run my program without providing the task argument, I'm gonna use this default value instead. So my default is going to be, "return a list of numbers." And we'll also tell it that we expect to receive an argument of "--language." I'll again provide a default here, I'll use the default of python. We'll then call parser.parse-args, and assign the result to args. So now this right here is gonna contain the parse command line arguments, and if none were provided, the defaults will be used instead. So we can now use these down when we run our chain. So down here at the very bottom, I'm gonna replace these hard-coded values with args.language and args.task. Let's save this, open up our terminal, now I'm gonna try to run my program with a python main.py and I'm gonna put in a language of how about javascript, and a task, and to put in a string here for the task, because we're gonna put in multiple words, we need to wrap it with single quotes. So I'll say a task of how about just 'print hello.' I'll run this and we should see very quickly, yep, definitely valid JavaScript code, does exactly what I want. I'm gonna also make sure that the default values work as well. So I'll do a python main.py, and it should revert back to that default behavior of making a Python program that's gonna return a list of numbers. All right, looks good.

The last big thing you have to do inside of our project is secure our API key. Remember, we do not like to store our API key as plain text inside of our project because anyone can easily view this key right here if we, say, upload our project to GitHub or something like that, and they can make requests on your behalf, which would cost you money. So we do not really wanna store this key as plain text inside of our project file. So here's what we're gonna do instead. First thing, a little bit of backstory. Whenever we call that OpenAI object right there, we get back this llm variable, behind the scenes, this object as a part of its initialization process is gonna look for a very specifically-named environment variable. It's gonna look for one with the name of exactly OpenAI_API_Key, all capitals. If that environment variable is defined, it's gonna use the value right there as its API key. So we're gonna take advantage of this fact. We need to define an environment variable with exactly that name that has our API key as its value. There are many different ways of setting up an environment variable. We're gonna use a method that is gonna work 100% of the time regardless of whether you are on Mac OS or Windows or anything else. So here's what we're gonna do. To define this environment variable, we're gonna first go into our project and define a file called dotenv, and this is gonna contain our environment variable name, which is OpenAI_API_Key, and we're going to assign to that our actual API key. Then, whenever we start up our project, we're gonna make use of a package called dotenv. This package gives us a function called load_dotenv, and whenever we call it, a little bit of magic is gonna happen behind the scenes. This function is going to go and find this file right here. It's again going to take a look at all the key value pairs inside there. It's gonna parse all that text, and it's going to set up a handful of different environment variables for us. So that's gonna give us a new environment variable of Open API, or excuse me, OpenAI_API_Key and give it a value of whatever we had assigned to that inside the dotenv file. Now, you'll notice that in this process I'm kind of still saying, "Oh yeah, we're gonna set up this "or create this file that's going to contain our API key." And it might sound like that does nothing different than what we're already doing inside of our project. The key here is that we usually do not commit this file to git or any kind of version control. Instead, we very purposefully ignore this file. So whenever we upload our project or push it to a remote or anything like that, we'll not be including in this file. And so we do not run the risk of accidentally exposing the secrets that are stored inside this file. Okay, so that's it. Let's go through the setup here really quickly. First, back at my terminal, we're going to install that package with a pip install, and then its name is actually python-dotenv. Once I've installed that, inside of my project at the very top, I'm gonna add in a from dotenv import load_dotenv. And then before I do anything else inside my program, I'm gonna call that load_dotenv function. So that's gonna find whatever dotenv file exists inside my project. It's gonna load it up, it's gonna parse it. It's gonna assign those different environment variables. Then, inside my project directory, I'll create the dotenv file itself. Inside there, we're gonna set up a very specifically-named environment variable of OpenAI_API_Key. And it's very critical here you make sure you have the exact same spelling as I. Has to be completely identical. And then we will assign to this our API key. So to get my API key, I'll go back over to main.py, scroll down a little bit and find my API key. Here it is right here. And I'm gonna put in just that value. We don't actually have to wrap it with quotes. You can if you want to, but it does not make a difference here. So I'm gonna save this. Then back over inside of main.py, we no longer have to have our API key here so I can delete all that. And we no longer have to pass in the API key manually. And that should be it. So let's now save this, open our terminal back up, do a quick test here. So I can run my program again. We'll do another language of Python and a task of "return list of numbers", and everything should continue to work. I should still see a result come back. And it looks like, yep, we're definitely good to go.

There's one last thing I would like to add into our program. Remember that one of the big goals of these chains that we put together is that we can take multiple chains and connect them together in interesting ways. So I wanna try that out. I wanna create a second chain. I want to wire it into or connect it to this first one and build something more interesting. So here's the goal. We looked at this diagram previously. I made a couple of changes to it. So we still are going to have our inputs. It's gonna go into the pipe, or excuse me, the chain that we already put together. It's kinda this pipeline step. We're going to generate some amount of code. We're then gonna take that generated code and I wanna feed it into a second chain. This second chain is gonna contain a template here, prompt template, that's gonna ask our language model to write a test for the code that was generated. We're gonna feed that completed prompt into our language model and then hopefully get a test out of this. So that's it. That's what we're going to do. Now, to set this up, we need to, of course, create a second chain. When we create this chain, we're gonna make sure that it's going to receive some inputs whose names correspond with the outputs of the previous chain. So Chain A is the one we already put together. This is our code chain. It's the thing that is generating some code. The outputs from this are language, task and text, which is the default. Using the default of text, maybe not super descriptive to other engineers. So maybe we should try to rename the output key here to something like code, just to make sure it's really clear to other engineers, "Hey, this chain is outputting something that can be kind of viewed as being code." We're then gonna take these outputs right here, and remember we're gonna kind of feed them over as inputs into the second chain. So then this second chain is gonna receive language and code. It's gonna toss them into the template, feed that off to the language model, and then it's gonna have outputs of language, code, 'cause that's what received as inputs, and it's generated test. We'll have it be text or we could rename it to test as well or whatever else. Okay, so a lot of work here, a lot of stuff to figure out. Let's just tackle this step by step. Step one in reality is to figure out how to take this default output key from our existing chain of text and rename it to be something more useful or meaningful to us, like code. This is gonna be pretty easy, so let's just take care of that right away. Back over, I'm gonna find my chain right here. I'm gonna pass in an additional configuration option of output_key, and I'll set that to "code". If I now save this file and rerun my program, I'll see that I still get this output dictionary and then I've got keys of language, task. And now instead of text, I've got code right there. Okay, so that's definitely a good first step. Now since we understand what's going on, quick pause, we'll come back and we're gonna build up our second chain.

The next thing we need to do is make our second chain. To make our second chain, we just need to make a second prompt template and then feed it into a brand-new chain. That's all, so pretty straightforward. The second prompt template that we're gonna put together is gonna take in inputs of language and code. So we just need to make sure we list those out as input variables, let's get to it. So back over here, I'm gonna find where I made my first prompt template and I'm going to co-locate my second prompt template right next to it. So I'm gonna kind of make sure all my templates are located in the same spot so I always know where to go if I ever need to change my template. I'll call this second prompt template test_prompt. It's not the best name 'cause it makes it sound like it's something that's not being used, but whatever, it's descriptive, it's exactly what it's doing inside of our program. So this will be my PromptTemplate. It's gonna have input variables of, let's double check that diagram just to make sure it's clear. So we need the language and the code from chain A. So I'm gonna have language and code, and then my template itself will be something like write test for the following language code, and then I'll put in a colon, I'm gonna put in a new line, and then the code itself. Okay, so there's my prompt template. Next, I'm gonna create the chain itself. Underneath the existing chain, I'll put in test chain. That will be an LLM chain as well. We're gonna reuse the exact same language model, we can use that same language model all over the place. Our prompt in this case will be the test_prompt. And then I'm going to rename the output key for the second chain as well rather than allowing it to use the default text, I'm gonna rename it to the test, once again, just to be really descriptive and help other engineers what the kind of meaning of this generated value is. So my output_key will be test. Okay, now, we're not done just yet. The next thing we need to do is take these two chains we've created and wire them together. As we do this, we really wanna take the output of chain A and feed it directly into the input of chain B. We refer to this as a sequential action. So we wanna take multiple different chains and run them in series and sequence or series them together, so to speak. So to do so, we're going to add in an additional import from LangChain to help us automatically take the output from one chain and feed it to the input of another. At the very top of the file, I'll add an import from LangChain.chains., oh, I've already got an import from there. So I'm gonna add in a second import from LangChain.chains of sequential chain. So we are building up another chain. It's gonna, internal constituents are gonna be the two chains we just put together. I'll then go back down. And I'm gonna call this one simply chain 'cause it's kind of everything put together. This will be a sequential chain. And we're gonna pass in a couple of different arguments to configure this thing and help understand that it's going to wire together the code chain and the test chain. So the first thing we'll do is list out the chains we want to run one after another. We want to first run the code chain and then we want to run the test chain. The input variables to this entire chain here, we have to list them all out. The input to the entire thing is gonna be our language and our task, and then the output from this entire process, the only things we really care about here are the code and the tests that were generated. So I'm gonna list those inputs to the entire two chains together and I'm gonna list out the two outputs that we really care about. I'll do this by putting in input variables of task and language and then output variables of test and code. And then finally, to run this, I'm gonna find code chain right here. I don't wanna run the code chain directly anymore, I want to run this big overall chain that combined everything together. So I'm gonna change that to chain. So we're now going to run the big SequentialChain. We're gonna feed in this initial language and task, and everything is pretty much exactly what we had before, so these are going to flow into chain A. We're then gonna get outputs of language, task, and code. Once again, these things are gonna kind of flow over to chain B, chain B is gonna run, and then out of all the outputs of chain B, we're only going to return code and test. Okay, so let's save this and run it. Now, remember, this is gonna take a little bit longer because we are running two different language models, one after another. So now here's our output, we still get this big dictionary, and if we kind of hunt around in here, we'll see that there's code right there and then there's test right there. So we generated both some code and a test for it as well. It's kind of hard to see what is going on right now, so I'm gonna do a little bit of formatting. I'm gonna change this. I'm gonna put in a print statement above. I'll do something like generated code and then result and print out just code. And then how about the same thing for generated test and then result and print out just the test. Okay, let's save that again and run it. And again, after a little bit of a pause here, 'cause we have to run the two models one after another, there we go, so now I've got my generated code, there's the function that was generated that accomplished my original task of return a list of numbers. And then I've got a test that was created right after it. And if everything is done correctly, the test should be at least somewhat realistic, it should actually kind of refer to the function that was generated. If I look closely, the name of the function that was generated was list_of_numbers. And if I look at the test, yep, it looks like it's testing list_of_numbers. So I'd say that this is a huge success. We've now got this two-part processing pipeline where we generate some information in chain A or our first chain over here, and then we feed on little bits of that generated stuff over to our second chain, and then we get our final output of just these values we actually care about. All right, this is fantastic, and I think we've learned a lot inside this project, but there's still a ton of stuff for us to do inside this course, so we are definitely not done yet.